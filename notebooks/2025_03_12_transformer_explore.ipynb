{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GPT style transformer from scratch\n",
    "1. This time just using the Karpathy codebase as a guide, not following it step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from trav_gpt import ROOT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import DictConfig, OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\"../conf\", version_base=None):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "cfg.paths.root = ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root': '/Users/traviswhitfield/Documents/github/trav_gpt', 'data': '${paths.root}/data', 'external': '${paths.data}/external', 'interim': '${paths.data}/interim', 'processed': '${paths.data}/processed', 'raw': '${paths.data}/raw'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/traviswhitfield/Documents/github/trav_gpt/data/external'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.paths.external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = Path(cfg.paths.external) / 'input.txt'\n",
    "\n",
    "with open(text_path, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer()\n",
    "tokenizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18,\n",
       " 47,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 1,\n",
       " 15,\n",
       " 47,\n",
       " 58,\n",
       " 47,\n",
       " 64,\n",
       " 43,\n",
       " 52,\n",
       " 10,\n",
       " 0,\n",
       " 14,\n",
       " 43,\n",
       " 44,\n",
       " 53,\n",
       " 56,\n",
       " 43,\n",
       " 1,\n",
       " 61,\n",
       " 43,\n",
       " 1,\n",
       " 54,\n",
       " 56,\n",
       " 53,\n",
       " 41,\n",
       " 43,\n",
       " 43,\n",
       " 42,\n",
       " 1,\n",
       " 39,\n",
       " 52,\n",
       " 63,\n",
       " 1,\n",
       " 44,\n",
       " 59,\n",
       " 56,\n",
       " 58,\n",
       " 46,\n",
       " 43,\n",
       " 56,\n",
       " 6,\n",
       " 1,\n",
       " 46,\n",
       " 43,\n",
       " 39,\n",
       " 56,\n",
       " 1,\n",
       " 51,\n",
       " 43,\n",
       " 1,\n",
       " 57,\n",
       " 54,\n",
       " 43,\n",
       " 39,\n",
       " 49,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 13,\n",
       " 50,\n",
       " 50,\n",
       " 10,\n",
       " 0,\n",
       " 31,\n",
       " 54,\n",
       " 43,\n",
       " 39,\n",
       " 49,\n",
       " 6,\n",
       " 1,\n",
       " 57,\n",
       " 54,\n",
       " 43,\n",
       " 39,\n",
       " 49,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 18,\n",
       " 47,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 1,\n",
       " 15,\n",
       " 47,\n",
       " 58,\n",
       " 47,\n",
       " 64,\n",
       " 43,\n",
       " 52,\n",
       " 10,\n",
       " 0,\n",
       " 37,\n",
       " 53,\n",
       " 59]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "\n",
    "TRAIN_RATIO = 0.9\n",
    "n = int(TRAIN_RATIO * len(data))\n",
    "\n",
    "# Split the data first into the train and test datasets\n",
    "# There's certainly a better way of doing this with textual data, but we'll do it like this for now.\n",
    "train = data[:n]\n",
    "test = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115394, 1003854, 111540)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the batches of data that I need should be in a single tensor object. They should just be <batch_size> different sets of text of <context_size> length\n",
    "# So I'll just randomly sample starting points in my giant, tokenized dataset and then grab the appropriate length vector from each of those locations\n",
    "# and stack them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, context_size = 8, batch_size = 4):\n",
    "    \"\"\"This will convert the input tensor (of the whole text) into the appropriate \n",
    "    inputs and target labels (x and y)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = train if split == 'train' else test\n",
    "\n",
    "    # Grab the starting points. This returns a tensor of shape (batch_size,)\n",
    "    ix = torch.randint(0, len(data) - context_size - 1, (batch_size, ))\n",
    "\n",
    "    # Once I've grabbed those starting points, then I need to just grab the contexts associated with\n",
    "    # each one (and also the targets, which will be shifted over by 1)\n",
    "    x = torch.stack([data[ix[i]: ix[i] + context_size] for i in range(batch_size)])\n",
    "    y = torch.stack([data[ix[i] + 1: ix[i] + context_size + 1] for i in range(batch_size)])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "x, y = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial network\n",
    "1. Let's start with just a simple multilayer perceptron (i.e. fully connected feedforward network)\n",
    "    - Can I just passed the tokenized inputs into this? It seems like that should work right?\n",
    "    - I can do that as long as I only pass in one input at a time I guess. \n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        # The embedding dim needs to be the same size as the vocab, because that's the\n",
    "        # output of this step. It should output the logit associated with each possible\n",
    "        # character. \n",
    "\n",
    "        # If I wanted to use a different embedding dimension, then I'd need to first\n",
    "        # embed the characters to that dimension, then have an additional step which\n",
    "        # generates the output logits associated with each character.\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings = vocab_size,\n",
    "                                                  embedding_dim = embed_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "\n",
    "        logits = self.token_embedding_table(x) # (B,T,E)\n",
    "        logits = self.fc1(logits) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            \n",
    "            # To calculate the loss across the whole batch, we just reshape the \n",
    "            # logits such that the batches are basically combined. Then we calculate the\n",
    "            # loss on each of the individual token predictions. \n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens = 50):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx) # (B,T,C) where B = batch size, T = context size, C = vocabulary size\n",
    "            \n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1) # Perform softmax on the C dimension\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_ITERS = 200\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3124, test loss 4.2986\n",
      "step 200: train loss 2.8225, test loss 2.8457\n",
      "step 400: train loss 2.6570, test loss 2.6771\n",
      "step 600: train loss 2.6349, test loss 2.6022\n",
      "step 800: train loss 2.5878, test loss 2.6145\n",
      "step 1000: train loss 2.6187, test loss 2.6179\n",
      "step 1200: train loss 2.5668, test loss 2.6191\n",
      "step 1400: train loss 2.5807, test loss 2.5988\n",
      "step 1600: train loss 2.5414, test loss 2.5583\n",
      "step 1800: train loss 2.5630, test loss 2.5593\n",
      "step 2000: train loss 2.5758, test loss 2.5694\n",
      "step 2200: train loss 2.5320, test loss 2.5770\n",
      "step 2400: train loss 2.5686, test loss 2.5289\n",
      "step 2600: train loss 2.5492, test loss 2.5800\n",
      "step 2800: train loss 2.5807, test loss 2.5579\n"
     ]
    }
   ],
   "source": [
    "EVAL_ITERS = 200\n",
    "LEARNING_RATE = 1e-2\n",
    "MAX_ITERS = 3000\n",
    "EMBED_SIZE = 10\n",
    "EVAL_INTERVAL = 300\n",
    "\n",
    "model = BigramLanguageModel(vocab_size=tokenizer.vocab_size, embed_size=EMBED_SIZE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(MAX_ITERS):\n",
    "\n",
    "    if iter % EVAL_ITERS == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, test loss {losses['val']:.4f}\")\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad() # zero out the previous gradients\n",
    "    loss.backward() # Backpropagate the loss through the NN\n",
    "    optimizer.step() # Update the model parameters using those gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I litak g, titovyolofad, s!? nowanthamere; spiarar\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(torch.zeros((1,1), dtype=torch.long))[0].tolist()\n",
    "\n",
    "print(tokenizer.decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trav_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GPT style transformer from scratch\n",
    "1. This time just using the Karpathy codebase as a guide, not following it step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from trav_gpt import ROOT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import DictConfig, OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\"../conf\", version_base=None):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "cfg.paths.root = ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root': '/Users/traviswhitfield/Documents/github/trav_gpt', 'data': '${paths.root}/data', 'external': '${paths.data}/external', 'interim': '${paths.data}/interim', 'processed': '${paths.data}/processed', 'raw': '${paths.data}/raw'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/traviswhitfield/Documents/github/trav_gpt/data/external'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.paths.external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = Path(cfg.paths.external) / 'input.txt'\n",
    "\n",
    "with open(text_path, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trav_gpt.gpt2 import CharTokenizer, get_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer()\n",
    "tokenizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18,\n",
       " 47,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 1,\n",
       " 15,\n",
       " 47,\n",
       " 58,\n",
       " 47,\n",
       " 64,\n",
       " 43,\n",
       " 52,\n",
       " 10,\n",
       " 0,\n",
       " 14,\n",
       " 43,\n",
       " 44,\n",
       " 53,\n",
       " 56,\n",
       " 43,\n",
       " 1,\n",
       " 61,\n",
       " 43,\n",
       " 1,\n",
       " 54,\n",
       " 56,\n",
       " 53,\n",
       " 41,\n",
       " 43,\n",
       " 43,\n",
       " 42,\n",
       " 1,\n",
       " 39,\n",
       " 52,\n",
       " 63,\n",
       " 1,\n",
       " 44,\n",
       " 59,\n",
       " 56,\n",
       " 58,\n",
       " 46,\n",
       " 43,\n",
       " 56,\n",
       " 6,\n",
       " 1,\n",
       " 46,\n",
       " 43,\n",
       " 39,\n",
       " 56,\n",
       " 1,\n",
       " 51,\n",
       " 43,\n",
       " 1,\n",
       " 57,\n",
       " 54,\n",
       " 43,\n",
       " 39,\n",
       " 49,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 13,\n",
       " 50,\n",
       " 50,\n",
       " 10,\n",
       " 0,\n",
       " 31,\n",
       " 54,\n",
       " 43,\n",
       " 39,\n",
       " 49,\n",
       " 6,\n",
       " 1,\n",
       " 57,\n",
       " 54,\n",
       " 43,\n",
       " 39,\n",
       " 49,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 18,\n",
       " 47,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 1,\n",
       " 15,\n",
       " 47,\n",
       " 58,\n",
       " 47,\n",
       " 64,\n",
       " 43,\n",
       " 52,\n",
       " 10,\n",
       " 0,\n",
       " 37,\n",
       " 53,\n",
       " 59]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "\n",
    "TRAIN_RATIO = 0.9\n",
    "n = int(TRAIN_RATIO * len(data))\n",
    "\n",
    "# Split the data first into the train and test datasets\n",
    "# There's certainly a better way of doing this with textual data, but we'll do it like this for now.\n",
    "train = data[:n]\n",
    "test = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115394, 1003854, 111540)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the batches of data that I need should be in a single tensor object. They should just be <batch_size> different sets of text of <context_size> length\n",
    "# So I'll just randomly sample starting points in my giant, tokenized dataset and then grab the appropriate length vector from each of those locations\n",
    "# and stack them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "x, y = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial network\n",
    "1. Let's start with just a simple multilayer perceptron (i.e. fully connected feedforward network)\n",
    "    - Can I just passed the tokenized inputs into this? It seems like that should work right?\n",
    "    - I can do that as long as I only pass in one input at a time I guess. \n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        # The embedding dim needs to be the same size as the vocab, because that's the\n",
    "        # output of this step. It should output the logit associated with each possible\n",
    "        # character. \n",
    "\n",
    "        # If I wanted to use a different embedding dimension, then I'd need to first\n",
    "        # embed the characters to that dimension, then have an additional step which\n",
    "        # generates the output logits associated with each character.\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings = vocab_size,\n",
    "                                                  embedding_dim = embed_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "\n",
    "        logits = self.token_embedding_table(x) # (B,T,E)\n",
    "        logits = self.fc1(logits) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            \n",
    "            # To calculate the loss across the whole batch, we just reshape the \n",
    "            # logits such that the batches are basically combined. Then we calculate the\n",
    "            # loss on each of the individual token predictions. \n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens = 50):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx) # (B,T,C) where B = batch size, T = context size, C = vocabulary size\n",
    "            \n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1) # Perform softmax on the C dimension\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_ITERS = 200\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3124, test loss 4.2986\n",
      "step 200: train loss 2.8225, test loss 2.8457\n",
      "step 400: train loss 2.6570, test loss 2.6771\n",
      "step 600: train loss 2.6349, test loss 2.6022\n",
      "step 800: train loss 2.5878, test loss 2.6145\n",
      "step 1000: train loss 2.6187, test loss 2.6179\n",
      "step 1200: train loss 2.5668, test loss 2.6191\n",
      "step 1400: train loss 2.5807, test loss 2.5988\n",
      "step 1600: train loss 2.5414, test loss 2.5583\n",
      "step 1800: train loss 2.5630, test loss 2.5593\n",
      "step 2000: train loss 2.5758, test loss 2.5694\n",
      "step 2200: train loss 2.5320, test loss 2.5770\n",
      "step 2400: train loss 2.5686, test loss 2.5289\n",
      "step 2600: train loss 2.5492, test loss 2.5800\n",
      "step 2800: train loss 2.5807, test loss 2.5579\n"
     ]
    }
   ],
   "source": [
    "EVAL_ITERS = 200\n",
    "LEARNING_RATE = 1e-2\n",
    "MAX_ITERS = 3000\n",
    "EMBED_SIZE = 10\n",
    "EVAL_INTERVAL = 300\n",
    "\n",
    "model = BigramLanguageModel(vocab_size=tokenizer.vocab_size, embed_size=EMBED_SIZE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(MAX_ITERS):\n",
    "\n",
    "    if iter % EVAL_ITERS == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, test loss {losses['val']:.4f}\")\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad() # zero out the previous gradients\n",
    "    loss.backward() # Backpropagate the loss through the NN\n",
    "    optimizer.step() # Update the model parameters using those gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I litak g, titovyolofad, s!? nowanthamere; spiarar\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(torch.zeros((1,1), dtype=torch.long))[0].tolist()\n",
    "\n",
    "print(tokenizer.decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpersistent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Add a buffer to the module.\n",
      "\n",
      "This is typically used to register a buffer that should not to be\n",
      "considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "is not a parameter, but is part of the module's state. Buffers, by\n",
      "default, are persistent and will be saved alongside parameters. This\n",
      "behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "only difference between a persistent buffer and a non-persistent buffer\n",
      "is that the latter will not be a part of this module's\n",
      ":attr:`state_dict`.\n",
      "\n",
      "Buffers can be accessed as attributes using given names.\n",
      "\n",
      "Args:\n",
      "    name (str): name of the buffer. The buffer can be accessed\n",
      "        from this module using the given name\n",
      "    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "        that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "        the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "    persistent (bool): whether the buffer is part of this module's\n",
      "        :attr:`state_dict`.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "    >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/trav_gpt/lib/python3.12/site-packages/torch/nn/modules/module.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model.register_buffer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0md_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnhead\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_encoder_layers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_decoder_layers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mactivation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mrelu\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x166c514e0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcustom_encoder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcustom_decoder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlayer_norm_eps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnorm_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A transformer model.\n",
      "\n",
      ".. note::\n",
      "    See `this tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n",
      "    for an in depth discussion of the performant building blocks PyTorch offers for building your own\n",
      "    transformer layers.\n",
      "\n",
      "User is able to modify the attributes as needed. The architecture\n",
      "is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer,\n",
      "Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\n",
      "Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information\n",
      "Processing Systems, pages 6000-6010.\n",
      "\n",
      "Args:\n",
      "    d_model: the number of expected features in the encoder/decoder inputs (default=512).\n",
      "    nhead: the number of heads in the multiheadattention models (default=8).\n",
      "    num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).\n",
      "    num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).\n",
      "    dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
      "    dropout: the dropout value (default=0.1).\n",
      "    activation: the activation function of encoder/decoder intermediate layer, can be a string\n",
      "        (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
      "    custom_encoder: custom encoder (default=None).\n",
      "    custom_decoder: custom decoder (default=None).\n",
      "    layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
      "    batch_first: If ``True``, then the input and output tensors are provided\n",
      "        as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
      "    norm_first: if ``True``, encoder and decoder layers will perform LayerNorms before\n",
      "        other attention and feedforward operations, otherwise after. Default: ``False`` (after).\n",
      "    bias: If set to ``False``, ``Linear`` and ``LayerNorm`` layers will not learn an additive\n",
      "        bias. Default: ``True``.\n",
      "\n",
      "Examples::\n",
      "    >>> transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
      "    >>> src = torch.rand((10, 32, 512))\n",
      "    >>> tgt = torch.rand((20, 32, 512))\n",
      "    >>> out = transformer_model(src, tgt)\n",
      "\n",
      "Note: A full example to apply nn.Transformer module for the word language model is available in\n",
      "https://github.com/pytorch/examples/tree/master/word_language_model\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/trav_gpt/lib/python3.12/site-packages/torch/nn/modules/transformer.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "nn.Transformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecoder_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'TransformerDecoderLayer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_layers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "TransformerDecoder is a stack of N decoder layers.\n",
      "\n",
      ".. note::\n",
      "    See `this tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n",
      "    for an in depth discussion of the performant building blocks PyTorch offers for building your own\n",
      "    transformer layers.\n",
      "\n",
      "Args:\n",
      "    decoder_layer: an instance of the TransformerDecoderLayer() class (required).\n",
      "    num_layers: the number of sub-decoder-layers in the decoder (required).\n",
      "    norm: the layer normalization component (optional).\n",
      "\n",
      "Examples::\n",
      "    >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
      "    >>> transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
      "    >>> memory = torch.rand(10, 32, 512)\n",
      "    >>> tgt = torch.rand(20, 32, 512)\n",
      "    >>> out = transformer_decoder(tgt, memory)\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/trav_gpt/lib/python3.12/site-packages/torch/nn/modules/transformer.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "nn.TransformerDecoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0md_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnhead\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mactivation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mrelu\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x166c514e0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlayer_norm_eps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnorm_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
      "\n",
      ".. note::\n",
      "    See `this tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n",
      "    for an in depth discussion of the performant building blocks PyTorch offers for building your own\n",
      "    transformer layers.\n",
      "\n",
      "This standard decoder layer is based on the paper \"Attention Is All You Need\".\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
      "Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
      "Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
      "in a different way during application.\n",
      "\n",
      "Args:\n",
      "    d_model: the number of expected features in the input (required).\n",
      "    nhead: the number of heads in the multiheadattention models (required).\n",
      "    dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
      "    dropout: the dropout value (default=0.1).\n",
      "    activation: the activation function of the intermediate layer, can be a string\n",
      "        (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
      "    layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
      "    batch_first: If ``True``, then the input and output tensors are provided\n",
      "        as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
      "    norm_first: if ``True``, layer norm is done prior to self attention, multihead\n",
      "        attention and feedforward operations, respectively. Otherwise it's done after.\n",
      "        Default: ``False`` (after).\n",
      "    bias: If set to ``False``, ``Linear`` and ``LayerNorm`` layers will not learn an additive\n",
      "        bias. Default: ``True``.\n",
      "\n",
      "Examples::\n",
      "    >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
      "    >>> memory = torch.rand(10, 32, 512)\n",
      "    >>> tgt = torch.rand(20, 32, 512)\n",
      "    >>> out = decoder_layer(tgt, memory)\n",
      "\n",
      "Alternatively, when ``batch_first`` is ``True``:\n",
      "    >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)\n",
      "    >>> memory = torch.rand(32, 10, 512)\n",
      "    >>> tgt = torch.rand(32, 20, 512)\n",
      "    >>> out = decoder_layer(tgt, memory)\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/trav_gpt/lib/python3.12/site-packages/torch/nn/modules/transformer.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "nn.TransformerDecoderLayer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Holds submodules in a list.\n",
      "\n",
      ":class:`~torch.nn.ModuleList` can be indexed like a regular Python list, but\n",
      "modules it contains are properly registered, and will be visible by all\n",
      ":class:`~torch.nn.Module` methods.\n",
      "\n",
      "Args:\n",
      "    modules (iterable, optional): an iterable of modules to add\n",
      "\n",
      "Example::\n",
      "\n",
      "    class MyModule(nn.Module):\n",
      "        def __init__(self) -> None:\n",
      "            super().__init__()\n",
      "            self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
      "\n",
      "        def forward(self, x):\n",
      "            # ModuleList can act as an iterable, or be indexed using ints\n",
      "            for i, l in enumerate(self.linears):\n",
      "                x = self.linears[i // 2](x) + l(x)\n",
      "            return x\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/trav_gpt/lib/python3.12/site-packages/torch/nn/modules/container.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     ParametrizationList"
     ]
    }
   ],
   "source": [
    "nn.ModuleList?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0min_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Applies an affine linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
      "\n",
      "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "Args:\n",
      "    in_features: size of each input sample\n",
      "    out_features: size of each output sample\n",
      "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "        Default: ``True``\n",
      "\n",
      "Shape:\n",
      "    - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "      dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "    - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "Attributes:\n",
      "    weight: the learnable weights of the module of shape\n",
      "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "            If :attr:`bias` is ``True``, the values are initialized from\n",
      "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> m = nn.Linear(20, 30)\n",
      "    >>> input = torch.randn(128, 20)\n",
      "    >>> output = m(input)\n",
      "    >>> print(output.size())\n",
      "    torch.Size([128, 30])\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/trav_gpt/lib/python3.12/site-packages/torch/nn/modules/linear.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     NonDynamicallyQuantizableLinear, LazyLinear, Linear, LinearBn1d, Linear"
     ]
    }
   ],
   "source": [
    "nn.Linear?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trav_gpt.gpt2 import GPTLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 32)\n",
      "  (position_embedding_table): Embedding(8, 32)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (mha): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (ffwd): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (mha): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (ffwd): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (mha): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (ffwd): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (mha): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (ffwd): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTFromPytorch(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0md_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnhead\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mactivation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mrelu\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x3104b2d40\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlayer_norm_eps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnorm_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
      "\n",
      ".. note::\n",
      "    See `this tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n",
      "    for an in depth discussion of the performant building blocks PyTorch offers for building your own\n",
      "    transformer layers.\n",
      "\n",
      "This standard decoder layer is based on the paper \"Attention Is All You Need\".\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
      "Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
      "Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
      "in a different way during application.\n",
      "\n",
      "Args:\n",
      "    d_model: the number of expected features in the input (required).\n",
      "    nhead: the number of heads in the multiheadattention models (required).\n",
      "    dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
      "    dropout: the dropout value (default=0.1).\n",
      "    activation: the activation function of the intermediate layer, can be a string\n",
      "        (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
      "    layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
      "    batch_first: If ``True``, then the input and output tensors are provided\n",
      "        as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
      "    norm_first: if ``True``, layer norm is done prior to self attention, multihead\n",
      "        attention and feedforward operations, respectively. Otherwise it's done after.\n",
      "        Default: ``False`` (after).\n",
      "    bias: If set to ``False``, ``Linear`` and ``LayerNorm`` layers will not learn an additive\n",
      "        bias. Default: ``True``.\n",
      "\n",
      "Examples::\n",
      "    >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
      "    >>> memory = torch.rand(10, 32, 512)\n",
      "    >>> tgt = torch.rand(20, 32, 512)\n",
      "    >>> out = decoder_layer(tgt, memory)\n",
      "\n",
      "Alternatively, when ``batch_first`` is ``True``:\n",
      "    >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)\n",
      "    >>> memory = torch.rand(32, 10, 512)\n",
      "    >>> tgt = torch.rand(32, 20, 512)\n",
      "    >>> out = decoder_layer(tgt, memory)\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/trav_gpt/lib/python3.12/site-packages/torch/nn/modules/transformer.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "nn.TransformerDecoderLayer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "chunk(input: Tensor, chunks: int, dim: int = 0) -> Tuple[Tensor, ...]\n",
      "\n",
      "Attempts to split a tensor into the specified number of chunks. Each chunk is a view of\n",
      "the input tensor.\n",
      "\n",
      "\n",
      ".. note::\n",
      "\n",
      "    This function may return fewer than the specified number of chunks!\n",
      "\n",
      ".. seealso::\n",
      "\n",
      "    :func:`torch.tensor_split` a function that always returns exactly the specified number of chunks\n",
      "\n",
      "If the tensor size along the given dimension :attr:`dim` is divisible by :attr:`chunks`,\n",
      "all returned chunks will be the same size.\n",
      "If the tensor size along the given dimension :attr:`dim` is not divisible by :attr:`chunks`,\n",
      "all returned chunks will be the same size, except the last one.\n",
      "If such division is not possible, this function may return fewer\n",
      "than the specified number of chunks.\n",
      "\n",
      "Arguments:\n",
      "    input (Tensor): the tensor to split\n",
      "    chunks (int): number of chunks to return\n",
      "    dim (int): dimension along which to split the tensor\n",
      "\n",
      "Example:\n",
      "    >>> torch.arange(11).chunk(6)\n",
      "    (tensor([0, 1]),\n",
      "     tensor([2, 3]),\n",
      "     tensor([4, 5]),\n",
      "     tensor([6, 7]),\n",
      "     tensor([8, 9]),\n",
      "     tensor([10]))\n",
      "    >>> torch.arange(12).chunk(6)\n",
      "    (tensor([0, 1]),\n",
      "     tensor([2, 3]),\n",
      "     tensor([4, 5]),\n",
      "     tensor([6, 7]),\n",
      "     tensor([8, 9]),\n",
      "     tensor([10, 11]))\n",
      "    >>> torch.arange(13).chunk(6)\n",
      "    (tensor([0, 1, 2]),\n",
      "     tensor([3, 4, 5]),\n",
      "     tensor([6, 7, 8]),\n",
      "     tensor([ 9, 10, 11]),\n",
      "     tensor([12]))\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "torch.chunk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Projection Time: 0.008170 sec\n",
      "Separate Projections Time: 0.006761 sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "embed_dim = 768\n",
    "batch_size = 16\n",
    "seq_len = 128\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "# Single projection (efficient way)\n",
    "class SingleProjection(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        qkv = self.qkv_proj(x)  # One matrix multiplication\n",
    "        return torch.chunk(qkv, 3, dim=-1)  # Split into Q, K, V\n",
    "\n",
    "# Separate projections (inefficient way)\n",
    "class SeparateProjections(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.q_proj(x)  # Three separate matrix multiplications\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        return q, k, v\n",
    "\n",
    "single_proj_model = SingleProjection(embed_dim)\n",
    "separate_proj_model = SeparateProjections(embed_dim)\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "q, k, v = single_proj_model(x)\n",
    "print(f\"Single Projection Time: {time.time() - start_time:.6f} sec\")\n",
    "\n",
    "start_time = time.time()\n",
    "q, k, v = separate_proj_model(x)\n",
    "print(f\"Separate Projections Time: {time.time() - start_time:.6f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trav_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
